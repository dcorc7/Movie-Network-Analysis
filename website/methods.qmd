---
title: Methods
---



# Neural Networks

Neural networks are flexible, data-driven models inspired by the structure and functioning of neurons in the human brain. These models consist of layers of interconnected nodes (neurons) that apply transformations to input data through weighted connections and nonlinear activation functions. A typical feedforward neural network comprises an input layer, one or more hidden layers, and an output layer. As data passes through the network, each neuron computes a weighted sum of its inputs, applies an activation function, and sends the result forward.

The network is trained by minimizing a loss function that quantifies prediction error. The most common optimization approach, backpropagation, is used to compute gradients of the loss with respect to each weight in the network. These gradients are then used to update the weights via gradient descent-based optimizers such as Adam, Stochastic Gradient Descent (SGD), or **RMSProp. By iteratively refining the weights, the model improves its performance on the training data and generalizes to new inputs.

To avoid overfitting, regularization techniques were employed, including dropout, which randomly disables a fraction of neurons during training, and early stopping, which halts training when validation performance plateaus. Manual hyperparameter tuning was conducted to identify the optimal configuration of layers, learning rate, batch size, and optimizer.

## Graph Convolutional Networks (GCN)



## Graph Attention Networks (GAT)


